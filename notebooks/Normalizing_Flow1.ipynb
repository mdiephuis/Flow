{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.distributions as D\n",
    "import torch.nn.functional as F\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(x_1, x_2) = \\mathcal{N}\\Big(x_1 |\\, \\mu= \\frac{1}{4 x^2_x}, \\sigma=1\\Big) \\cdot \\mathcal{N}\\Big(x_2 |\\, \\mu=0, \\sigma=4\\Big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "# Draw batch_size samples from first distribution\n",
    "loc2, scale2 = torch.tensor([0.0]), torch.tensor([4.0])\n",
    "dist2 = D.Normal(loc2, scale2)\n",
    "x2 = dist2.sample((batch_size,))\n",
    "\n",
    "# Get a point estimate for all x_2 samples\n",
    "loc1 = 0.25 * torch.pow(x2, 2)\n",
    "scale1 = torch.ones_like(loc1)\n",
    "dist1 = D.Normal(loc1, scale1)\n",
    "x1 = dist1.sample()\n",
    "\n",
    "x_samples = torch.stack((x1, x2), dim=0).squeeze(-1).transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_samples[:, 0], x_samples[:, 1], s=10, color='red')\n",
    "plt.xlim([-5, 30])\n",
    "plt.ylim([-10, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT = D.transforms.AffineTransform(torch.zeros((1, 2)), torch.tensor(1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = AT(x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y[:, 0], y[:, 1], s=10, color='blue')\n",
    "plt.scatter(x_samples[:, 0], x_samples[:, 1], s=10, color='red')\n",
    "plt.xlim([-5, 60])\n",
    "plt.ylim([-20, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the flow\n",
    "\n",
    "Flow that resembles a standard fully-connected network, so alternating matrix multiplication with non-linearities\n",
    "\n",
    "Determinants are computationally expensive, use matrix determinant lemma and a structured affine transform. The latter is parameterized as a lower triangular matrix $M$ and a low rank update:\n",
    "$$M + VDV^T$$\n",
    "\n",
    "Next, we need an invertible nonlinearity. Sigmoids and tanh functions are incredibly unstable to invert as small changes in output near -1, 1 correspond to massive changes in the input. ReLu is stable, but not invertible for $x \\le 0$.\n",
    "\n",
    "Hence, PReLU, Parameterized ReLU, which is like leaky ReLU, but with a learnable slope in the negative regime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Custom non linearity.\n",
    "# class ParLeakyReluFunction(torch.autograd.Function):\n",
    "\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, input, alpha):\n",
    "#         ctx.save_for_backward(input, alpha)\n",
    "#         output = torch.where(input >= 0, input, input * alpha)\n",
    "#         return output\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         input, alpha = ctx.saved_tensors\n",
    "#         grad_input = grad_weight = grad_bias = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base distribution\n",
    "base_dist = D.multivariate_normal.MultivariateNormal(torch.zeros(2), torch.eye(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParLeakyRelu(nn.Module):\n",
    "    def __init__(self, alpha, event_dims):\n",
    "        super(ParLeakyRelu, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.event_dim = event_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.where(x >= 0, x, x * self.alpha)\n",
    "        \n",
    "    def inverse(self, x):\n",
    "        return torch.where(x >=0, x, 1. / (self.alpha * x))\n",
    "    \n",
    "    def inverse_log_det_jacobian(self, x):\n",
    "        I = torch.ones_like(x)\n",
    "        J_inv = torch.where(y >= 0, I, 1.0 / (self.alpha * I))\n",
    "        # Determine log abs det of J_inv\n",
    "        log_abs_det_J_inv = torch.log(torch.abs(J_inv))\n",
    "        return torch.sum(log_abs_det_J_inv, dim=self.event_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBijector(nn.Module):\n",
    "    def __init__(self, alpha, d, r, num_layers):\n",
    "        super(MLPBijector, self).__init__()\n",
    "        self.d = d\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = nn.ModuleList([\n",
    "            D.transforms.AffineTransform(), \n",
    "            ParLeakyRelu(self.alpha)\n",
    "        ])\n",
    "        self.output_layer = D.transforms.AffineTransform()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = encoder(x)\n",
    "        return self.output_layer(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AT = D.transforms.AffineTransform(torch.zeros((1, 2)), torch.tensor(1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, x_hat):\n",
    "    nll_loss = nn.NLLLoss(x_hat, x, reduction=mean)\n",
    "    return nll_loss\n",
    "\n",
    "def get_optimizer(model):\n",
    "    lr = 1e-3\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,\n",
    "                     betas=(beta1, beta2))\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def init_weights(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose2d):\n",
    "            init.xavier_uniform_(m.weight.data)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias, 0.0)\n",
    "        elif isinstance(m, nn.Sequential):\n",
    "            for sub_mod in m:\n",
    "                init_weights(sub_mod)\n",
    "\n",
    "\n",
    "alpha = 0.8\n",
    "d = 2 \n",
    "r = 2\n",
    "num_layers = 6\n",
    "\n",
    "model = MLPBijector(alpha, d, r, num_layers)\n",
    "init_weights(model)\n",
    "opt = get_optimizer(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo, custom layer, or\n",
    "# nn.Linear for Affine transform, since its the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
