{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.distributions as distrib\n",
    "import torch.distributions.transforms as transform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack\n",
    "# ref:\n",
    "class Flow(transform.Transform, nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        transform.Transform.__init__(self)\n",
    "        nn.Module.__init__(self)\n",
    "    \n",
    "    # Init all parameters\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.01, 0.01)\n",
    "            \n",
    "    # Hacky hash bypass\n",
    "    def __hash__(self):\n",
    "        return nn.Module.__hash__(self)\n",
    "    \n",
    "    \n",
    "class PlanarFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.scale = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(1))\n",
    "        self.init_parameters()\n",
    "\n",
    "    def _call(self, z):\n",
    "        f_z = F.linear(z, self.weight, self.bias)\n",
    "        return z + self.scale * torch.tanh(f_z)\n",
    "\n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        f_z = F.linear(z, self.weight, self.bias)\n",
    "        psi = (1 - torch.tanh(f_z) ** 2) * self.weight\n",
    "        det_grad = 1 + torch.mm(psi, self.scale.t())\n",
    "        return torch.log(det_grad.abs() + 1e-9)\n",
    "    \n",
    "    \n",
    "# Main class for normalizing flow\n",
    "class NormalizingFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, blocks, flow_length, density):\n",
    "        super().__init__()\n",
    "        biject = []\n",
    "        for f in range(flow_length):\n",
    "            for b_flow in blocks:\n",
    "                biject.append(b_flow(dim))\n",
    "        self.transforms = transform.ComposeTransform(biject)\n",
    "        self.bijectors = nn.ModuleList(biject)\n",
    "        self.base_density = density\n",
    "        self.final_density = distrib.TransformedDistribution(density, self.transforms)\n",
    "        self.log_det = []\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.log_det = []\n",
    "        # Applies series of flows\n",
    "        for b in range(len(self.bijectors)):\n",
    "            self.log_det.append(self.bijectors[b].log_abs_det_jacobian(z))\n",
    "            z = self.bijectors[b](z)\n",
    "        return z, self.log_det\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes, encoder_dim, decoder_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim  = input_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Linear(self.input_dim, self.encoder_dim),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(self.encoder_dim, self.encoder_dim),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(self.encoder_dim, self.encoder_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder network\n",
    "        self.decoder = nn.Sequential(\n",
    "                    nn.Linear(self.latent_dim, self.decoder_dim),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(self.decoder_dim, self.decoder_dim),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(self.decoder_dim, self.input_dim * n_classes),\n",
    "                    nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(self.encoder_dim, self.latent_dim)\n",
    "        self.sigma = nn.Sequential(\n",
    "                    nn.Linear(self.encoder_dim, self.latent_dim),\n",
    "                    nn.Softplus(),\n",
    "                    nn.Hardtanh(min_val=1e-4, max_val=5.)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu_z = self.mu(x)\n",
    "        sigma_z = self.sigma(x)\n",
    "        return mu_z, sigma_z\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def reparameterize(self, x, mu_z, sigma_z):\n",
    "        eps = torch.randn_like(sigma_z)\n",
    "        z = eps.mul(sigma_z) + mu_z\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        kl_div = -0.5 * torch.sum(1 + sigma_z - mu_z.pow(2) - sigma_z.exp())\n",
    "        kl_div = kl_div / batch_size\n",
    "        return z, kl_div\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu_z, sigma_z = self.encode(x)\n",
    "        z_hat, kl_div = self.reparameterize(x, mu_z, sigma_z)\n",
    "        x_hat = self.decode(z_hat)\n",
    "        return x_hat, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_loss(x_hat, x):\n",
    "    return F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "\n",
    "def multinomial_loss(x_logit, x):\n",
    "    batch_size = x.shape[0]\n",
    "    # Reshape input\n",
    "    x_logit = x_logit.view(batch_size, num_classes, x.shape[1], x.shape[2], x.shape[3])\n",
    "    # Take softmax\n",
    "    x_logit = F.log_softmax(x_logit, 1)\n",
    "    # make integer class labels\n",
    "    target = (x * (num_classes - 1)).long()\n",
    "    # computes cross entropy over all dimensions separately:\n",
    "    ce = F.nll_loss(x_logit, target, weight=None, reduction='none')\n",
    "    return ce.sum(dim = 0)*100\n",
    "\n",
    "def reconstruction_loss(x_tilde, x, num_classes=1, average=True):\n",
    "    if (num_classes == 1):\n",
    "        loss = binary_loss(x_tilde, x.view(x.size(0), -1))\n",
    "    else:\n",
    "        loss = multinomial_loss(x_tilde, x)\n",
    "    if (average):\n",
    "        loss = loss.sum() / x.size(0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print('GPU')\n",
    "else:\n",
    "    use_cuda = False\n",
    "    dtype = torch.FloatTensor\n",
    "    device = torch.device(\"CPU\")\n",
    "    \n",
    "    \n",
    "def to_cuda(tensor):\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "batch_size = 64\n",
    "tens_t = T.ToTensor()\n",
    "\n",
    "train_dset = datasets.FashionMNIST('data', train=True, download=True, transform=tens_t)\n",
    "train_loader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_dset = datasets.FashionMNIST('data', train=False, transform=tens_t)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(batch, nslices=8):\n",
    "    # Create one big image for plot\n",
    "    img = np.zeros(((batch.shape[2] + 1) * nslices, (batch.shape[3] + 1) * nslices))\n",
    "    for b in range(batch.shape[0]):\n",
    "        row = int(b / nslices); col = int(b % nslices)\n",
    "        r_p = row * batch.shape[2] + row; c_p = col * batch.shape[3] + col\n",
    "        img[r_p:(r_p+batch.shape[2]),c_p:(c_p+batch.shape[3])] = torch.sum(batch[b], 0)\n",
    "    im = plt.imshow(img, cmap='Greys', interpolation='nearest'),\n",
    "    return im\n",
    "# Select a random set of fixed data\n",
    "fixed_batch, fixed_targets = next(iter(test_loader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_batch(fixed_batch);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, optimizer, scheduler, train_loader, model_name='basic', epochs=50, plot_it=1, flatten=True, use_cuda=True):\n",
    "    # Losses curves\n",
    "    losses = torch.zeros(epochs, 2)\n",
    "    # Beta-warmup\n",
    "    beta = 0\n",
    "    # Plotting\n",
    "    ims = []\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    # Main optimization loop\n",
    "    for it in range(epochs):\n",
    "        it_loss = torch.Tensor([2])\n",
    "        # Update our beta\n",
    "        beta = 1. * (it / float(epochs))\n",
    "        n_batch = 0.\n",
    "        # Evaluate loss and backprop\n",
    "        for batch_idx, (x, _) in enumerate(train_loader):\n",
    "            x = x.cuda() if use_cuda else x\n",
    "            # Flatten input data\n",
    "            if (flatten):\n",
    "                x = x.view(-1, x.size(2) * x.size(3))\n",
    "            # Pass through VAE\n",
    "            x_tilde, loss_latent = model(x)\n",
    "            # Compute reconstruction loss\n",
    "            loss_recons = reconstruction_loss(x_tilde, x, num_classes)\n",
    "            # Evaluate loss and backprop\n",
    "            loss = loss_recons + (beta * loss_latent)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            losses[it, 0] += loss_recons.item()\n",
    "            losses[it, 1] += loss_latent.item()\n",
    "            n_batch += 1.\n",
    "        losses[it, :] /= n_batch    \n",
    "#         if (it % plot_it == 0):\n",
    "#             # Encode our fixed batch\n",
    "#             x_test, _  = next(iter(test_loader))\n",
    "#             x_test = x_test.cuda() if use_cuda else x_test\n",
    "#             if (flatten):\n",
    "#                 x_test = x_test.view(-1, x_test.size(2) * x_test.size(3))\n",
    "#             x_tilde, _ = model(x_test)\n",
    "            \n",
    "#             if (num_classes > 1):\n",
    "#                 # Find largest class logit\n",
    "#                 tmp = x_tilde.view(-1, num_classes, *x[0].shape[1:]).max(dim=1)[1]\n",
    "#                 x_tilde = tmp.float() / (num_classes - 1.)\n",
    "                \n",
    "#             ims.append(plot_batch(x_tilde.cpu().detach().view(-1, 1, 28, 28)))\n",
    "#             plt.title('Iter.%i'%(it), fontsize=15);\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model def and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bernoulli or Multinomial loss\n",
    "num_classes = 1\n",
    "# Number of hidden and latent\n",
    "n_hidden = 512\n",
    "n_latent = 2\n",
    "\n",
    "# Compute input dimensionality\n",
    "nin = fixed_batch.shape[2] * fixed_batch.shape[3]\n",
    "\n",
    "# Build the VAE model\n",
    "model = VAE(nin, num_classes, n_hidden, n_hidden, n_latent).type(dtype)\n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Launch our optimization\n",
    "losses_kld = train_vae(model, optimizer, scheduler, train_loader, model_name='basic', epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLL for generative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special as ss\n",
    "\n",
    "def evaluate_nll_bpd(data_loader, model, batch_size = 500, R = 5, use_cuda=True):\n",
    "    model.eval()\n",
    "    # Set of likelihood tests\n",
    "    likelihood_test = []\n",
    "    # Go through dataset\n",
    "    for batch_idx, (x, _) in enumerate(data_loader):\n",
    "        x = x.cuda() if use_cuda else x\n",
    "        for j in range(x.shape[0]):\n",
    "            a = []\n",
    "            for r in range(0, R):\n",
    "                cur_x = x[j].unsqueeze(0)\n",
    "                # Repeat it as batch\n",
    "                x = cur_x.expand(batch_size, *cur_x.size()[1:]).contiguous()\n",
    "                x = x.view(batch_size, -1)\n",
    "                x_tilde, kl_div = model(x)\n",
    "                rec = reconstruction_loss(x_tilde, x, average=False)\n",
    "                a_tmp = (rec + kl_div)\n",
    "                a.append(- a_tmp.cpu().data.numpy())\n",
    "            # calculate max\n",
    "            a = np.asarray(a)\n",
    "            a = a[:, np.newaxis]\n",
    "            likelihood_x = ss.logsumexp(a)\n",
    "            likelihood_test.append(likelihood_x - np.log(len(a)))\n",
    "    likelihood_test = np.array(likelihood_test)\n",
    "    nll = - np.mean(likelihood_test)\n",
    "    # Compute the bits per dim (but irrelevant for binary data)\n",
    "    bpd = nll / (np.prod(nin) * np.log(2.))\n",
    "    return nll, bpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final loss\n",
    "plt.figure()\n",
    "plt.plot(losses_kld[:, 0].numpy());\n",
    "# Evaluate log-likelihood and bits per dim\n",
    "nll, _ = evaluate_nll_bpd(test_loader, model)\n",
    "print('Negative Log-Likelihood : ' + str(nll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
