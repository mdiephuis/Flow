{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing flows in PyTorch (part 3)\n",
    "## Flows in variational inference\n",
    "\n",
    "The implementation of *Normalizing Flows* [[1](#reference1),[2](#reference)] based on `torch.distributions` that we saw in the two previous tutorials was intentionnaly constrained to toy examples, where we only optimized the transform from a Gaussian to different given *target densities*. However, flows come in very handy in several probabilistic inference mecanisms, and notably in *variational inference*. The main idea is still to *transform* a simple probability distribution through a sequence of invertible nonlinear transforms. However, this optimization will be part of a larger scheme of probabilistic inference. \n",
    "\n",
    "In this tutorial we will cover\n",
    "1. A [quick recap](#recap) of the concepts previously introduced (smaller than before)\n",
    "2. A fast introduction to [Variational Auto-Encoders](#vae)\n",
    "3. An explanation of how [normalizing flows improve VAEs](#nf) and how to implement it\n",
    "4. Some [modifications and tips to improve reconstruction](#improve) while combining NFs and VAEs\n",
    "\n",
    "**Note**: Once again, this tutorial is a work-in-progress, so I would gladly take any feedback, update or suggestions. If you have any ideas, please contact me (https://esling.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"recap\"></a>\n",
    "## Quick recap\n",
    "\n",
    "### Normalizing flows\n",
    "\n",
    "Normalizing flows allow to transform a probability distribution, through a series of *change of variable*. If we start with a random vector $\\mathbf{z}_0$ with distribution $q_0$, we can apply a series of mappings $f_i$, $i \\in 1,\\cdots,k$ with $k\\in\\mathcal{N}^{+}$ and obtain a normalizing flow. If we apply $k$ normalizing flows, the distribution of $\\mathbf{z}_k\\sim q_k(\\mathbf{z}_k)$ in log-probability will be given by\n",
    "\n",
    "$$\n",
    "\\text{log} q_K(\\mathbf{z}_k) = \\text{log} q_0(\\mathbf{z}_0) - \\sum_{i=1}^{k} \\text{log} \\left|\\text{det}\\frac{\\delta f_i}{\\delta\\mathbf{z}_{i-1}}\\right| \n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "Of course, we can perform any amount of combined transformations.\n",
    "\n",
    "### PyTorch distributions\n",
    "\n",
    "We still rely on the novel [PyTorch distributions module](https://pytorch.org/docs/stable/_modules/torch/distributions/), which is defined in `torch.distributions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as distrib\n",
    "import torch.distributions.transforms as transform\n",
    "import matplotlib.animation as animation\n",
    "matplotlib.rcParams['animation.ffmpeg_path'] = '/usr/local/bin/ffmpeg'\n",
    "from IPython.display import HTML\n",
    "# Imports for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Define grids of points (for later plots)\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "z = np.array(np.meshgrid(x, x)).transpose(1, 2, 0)\n",
    "z = np.reshape(z, [z.shape[0] * z.shape[1], -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing normalizing flows\n",
    "\n",
    "The main interest in normalizing flows is that we could optimize the parameters of these flow in order to fit complex and rich probability distributions. In order to perform *inference*, we had to deal with the fact that the `Transform` object is not inherently parametric. To do so, we define our own `Flow` class which can be seen both as a `Transform` and also a `Module`that can be optmized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow(transform.Transform, nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        transform.Transform.__init__(self)\n",
    "        nn.Module.__init__(self)\n",
    "    \n",
    "    # Init all parameters\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.01, 0.01)\n",
    "            \n",
    "    # Hacky hash bypass\n",
    "    def __hash__(self):\n",
    "        return nn.Module.__hash__(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this minimal class, we defined a wide variety of flows in the previous tutorials, but we will here simply reuse the *planar* and *radial* flows. Therefore, we redefine here the `PlanarFlow` and `RadialFlow` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.scale = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(1))\n",
    "        self.init_parameters()\n",
    "\n",
    "    def _call(self, z):\n",
    "        f_z = F.linear(z, self.weight, self.bias)\n",
    "        return z + self.scale * torch.tanh(f_z)\n",
    "\n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        f_z = F.linear(z, self.weight, self.bias)\n",
    "        psi = (1 - torch.tanh(f_z) ** 2) * self.weight\n",
    "        det_grad = 1 + torch.mm(psi, self.scale.t())\n",
    "        return torch.log(det_grad.abs() + 1e-9)\n",
    "    \n",
    "class RadialFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(RadialFlow, self).__init__()\n",
    "        self.z0 = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.alpha = nn.Parameter(torch.Tensor(1))\n",
    "        self.beta = nn.Parameter(torch.Tensor(1))\n",
    "        self.dim = dim\n",
    "        self.init_parameters()\n",
    "\n",
    "    def _call(self, z):\n",
    "        r = torch.norm(z - self.z0, dim=1).unsqueeze(1)\n",
    "        h = 1 / (self.alpha + r)\n",
    "        return z + (self.beta * h * (z - self.z0))\n",
    "\n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        r = torch.norm(z - self.z0, dim=1).unsqueeze(1)\n",
    "        h = 1 / (self.alpha + r)\n",
    "        hp = - 1 / (self.alpha + r) ** 2\n",
    "        bh = self.beta * h\n",
    "        det_grad = ((1 + bh) ** self.dim - 1) * (1 + bh + self.beta * hp * r)\n",
    "        return torch.log(det_grad.abs() + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to define a sequence of such flows, we defined a `NormalizingFlow` class, which is responsible for applying a series of flows and recording their determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main class for normalizing flow\n",
    "class NormalizingFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, blocks, flow_length, density):\n",
    "        super().__init__()\n",
    "        biject = []\n",
    "        for f in range(flow_length):\n",
    "            for b_flow in blocks:\n",
    "                biject.append(b_flow(dim))\n",
    "        self.transforms = transform.ComposeTransform(biject)\n",
    "        self.bijectors = nn.ModuleList(biject)\n",
    "        self.base_density = density\n",
    "        self.final_density = distrib.TransformedDistribution(density, self.transforms)\n",
    "        self.log_det = []\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.log_det = []\n",
    "        # Applies series of flows\n",
    "        for b in range(len(self.bijectors)):\n",
    "            self.log_det.append(self.bijectors[b].log_abs_det_jacobian(z))\n",
    "            z = self.bijectors[b](z)\n",
    "        return z, self.log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vae\"></a>\n",
    "## Variational auto-encoders\n",
    "\n",
    "An exhaustive explanation on Variational Auto-Encoders (VAEs) is out of the scope of this tutorial, so refer to the original papers [[3](#reference1),[4](#reference)] for a complete understanding. Briefly, VAEs are generative models aiming to find the underlying probability distribution of the data $p(\\mathbf{x})$ based on a set of examples in $\\mathbf{x}\\in\\mathbb{R}^{d_{x}}$. To do so, we consider *latent variables* defined in a lower-dimensional space $\\mathbf{z}\\in\\mathbb{R}^{d_{z}}$ ($d_{z} \\ll d_{x}$) with the joint probability distribution $p(\\mathbf{x}, \\mathbf{z}) = p(\\mathbf{x} \\vert \\mathbf{z})p(\\mathbf{z})$. Unfortunately, for complex distributions this integral is too complex and cannot be found in closed form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Variational inference\n",
    "\n",
    "The idea of *variational inference* (VI) allows to solve this problem through *optimization* by assuming a simpler approximate distribution $q_{\\phi}(\\mathbf{z}\\vert\\mathbf{x})\\in\\mathcal{Q}$ from a family $\\mathcal{Q}$ of approximate densities. Hence, the goal is to minimize the difference between this approximation and the real distribution. Therefore, this turns into the optimization problem of minimizing the Kullback-Leibler (KL) divergence between the parametric approximation and the original density\n",
    "\n",
    "$$\n",
    "q_{\\phi}^{*}(\\mathbf{z}\\vert \\mathbf{x})=\\text{argmin}_{q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x})\\in\\mathcal{Q}} \\mathcal{D}_{KL} \\big[ q_{\\phi}\\left(\\mathbf{z} \\vert \\mathbf{x}\\right) \\parallel p\\left(\\mathbf{z} \\vert \\mathbf{x}\\right) \\big]\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "By developing this KL divergence and re-arranging terms (the detailed development can be found in [3](#reference1)), we obtain\n",
    "\n",
    "$$\n",
    "\\log{p(\\mathbf{x})} - D_{KL} \\big[ q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x}) \\parallel p(\\mathbf{z} \\vert \\mathbf{x}) \\big] =\n",
    "\\mathbb{E}_{\\mathbf{z}} \\big[ \\log{p(\\mathbf{x} \\vert \\mathbf{z})}\\big] - D_{KL} \\big[ q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x}) \\parallel p(\\mathbf{z}) \\big]\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "This formulation describes the quantity we want to maximize $\\log p(\\mathbf{x})$ minus the error we make by using an approximate $q$ instead of $p$. Therefore, we can optimize this alternative objective, called the *evidence lower bound* (ELBO)\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\theta, \\phi} = \\mathbb{E} \\big[ \\log{ p_\\theta (\\mathbf{x|z}) } \\big] - \\beta \\cdot D_{KL} \\big[ q_\\phi(\\mathbf{z|x}) \\parallel p_\\theta(\\mathbf{z}) \\big]\n",
    "\\end{equation}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "We can see that this equation involves $q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x})$ which *encodes* the data $\\mathbf{x}$ into the latent representation $\\mathbf{z}$ and a *decoder* $p(\\mathbf{x} \\vert \\mathbf{z})$, which allows generating a data vector $\\mathbf{x}$ given a latent configuration $\\mathbf{z}$. Hence, this structure defines the *Variational Auto-Encoder* (VAE).\n",
    "\n",
    "The VAE objective can be interpreted intuitively. The first term  increases the likelihood of the data generated given a configuration of the latent, which amounts to minimize the *reconstruction error*. The second term represents the error made by using a simpler posterior distribution $q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x})$ compared to the true prior $p_{\\theta}(\\mathbf{z})$. Therefore, this *regularizes* the choice of approximation $q$ so that it remains close to the true posterior distribution [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparametrization trick\n",
    "\n",
    "Now, while this formulation has some very interesting properties, it involves sampling operations, where we need to draw the latent point $\\mathbf{z}$ from the distribution $q_{\\phi}(\\mathbf{z}\\vert\\mathbf{x})$.  The simplest choice for this variational approximate posterior is a multivariate Gaussian with a diagonal covariance structure (which leads to independent Gaussians on every dimension, called the *mean-field* family) so that\n",
    "$$\n",
    "\\text{log}q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) = \\text{log}\\mathcal{N}(\\mathbf{z};\\mathbf{\\mu}^{(i)},\\mathbf{\\sigma}^{(i)})\n",
    "\\tag{5}\n",
    "$$\n",
    "where the mean $\\mathbf{\\mu}^{(i)}$ and standard deviation $\\mathbf{\\sigma}^{(i)}$ of the approximate posterior are different for each input point and are produced by our encoder parametrized by its variational parameters $\\phi$. Now the KL divergence between this distribution and a simple prior $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ can be very simply obtained with\n",
    "$$\n",
    "D_{KL} \\big[ q_\\phi(\\mathbf{z|x}) \\parallel \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\big] = \\frac{1}{2}\\sum_{j=1}^{D}\\left(1+\\text{log}((\\sigma^{(i)}_j)^2)+(\\mu^{(i)}_j)^2+(\\sigma^{(i)}_j)^2\\right)\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "While this looks convenient, we will still have to perform gradient descent through a sampling operation, which is non-differentiable. To solve this issue, we can use the *reparametrization trick*, which takes the sampling operation outside of the gradient flow by considering $\\mathbf{z}^{(i)}=\\mathbf{\\mu}^{(i)}+\\mathbf{\\sigma}^{(i)}\\odot\\mathbf{\\epsilon}^{(l)}$ with $\\mathbf{\\epsilon}^{(l)}\\sim\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimalist VAE implementation\n",
    "\n",
    "As we have seen, VAEs can be simply implemented by decomposing the above series of operations into an `encoder` which represents the distribution $q_\\phi(\\mathbf{z}\\vert\\mathbf{x})$, from which we will sample some values $\\tilde{\\mathbf{z}}$ (using the reparametrization trick) and compute the Kullback-Leibler (KL) divergence. Then, we use these values as input to a `decoder` which represents the distribution $p_\\theta(\\mathbf{x}\\vert\\mathbf{z})$ so that we can produce a reconstruction $\\tilde{\\mathbf{x}}$ and compute the reconstruction error. This process is implemented in the following `VAE` class.\n",
    "\n",
    "Note that we purposedly rely on an implementation of the `encode` function where the `encoder` first produces an intermediate representation of size `encoder_dims`. Then, this representation goes through two separate functions for encoding $\\mathbf{\\mu}$ and $\\mathbf{\\sigma}$. This provides a clearer implementation but also the added bonus that we can ensure that $\\mathbf{\\sigma} > 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, encoder_dims, latent_dims):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.latent_dims = latent_dims\n",
    "        self.encoder_dims = encoder_dims\n",
    "        self.mu = nn.Linear(encoder_dims, latent_dims)\n",
    "        self.sigma = nn.Sequential(\n",
    "            nn.Linear(encoder_dims, latent_dims),\n",
    "            nn.Softplus(),\n",
    "            nn.Hardtanh(min_val=1e-4, max_val=5.))\n",
    "        self.apply(self.init_parameters)\n",
    "    \n",
    "    def init_parameters(self, m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        return mu, sigma\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode the inputs\n",
    "        z_params = self.encode(x)\n",
    "        # Obtain latent samples and latent loss\n",
    "        z_tilde, kl_div = self.latent(x, z_params)\n",
    "        # Decode the samples\n",
    "        x_tilde = self.decode(z_tilde)\n",
    "        return x_tilde, kl_div\n",
    "    \n",
    "    def latent(self, x, z_params):\n",
    "        n_batch = x.size(0)\n",
    "        # Retrieve mean and var\n",
    "        mu, sigma = z_params\n",
    "        # Re-parametrize\n",
    "        q = distrib.Normal(torch.zeros(mu.shape[1]), torch.ones(sigma.shape[1]))\n",
    "        z = (sigma * q.sample((n_batch, ))) + mu\n",
    "        # Compute KL divergence\n",
    "        kl_div = -0.5 * torch.sum(1 + sigma - mu.pow(2) - sigma.exp())\n",
    "        kl_div = kl_div / n_batch\n",
    "        return z, kl_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the interesting aspect of VAEs is that we can define any parametric function as `encoder` and `decoder`, as long as we can optimize them. Here, we will rely on simple feed-forward neural networks, but these can be largely more complex (with limitations that we will discuss later in the tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encoder_decoder(nin, n_latent = 16, n_hidden = 512, n_classes = 1):\n",
    "    # Encoder network\n",
    "    encoder = nn.Sequential(\n",
    "                nn.Linear(nin, n_hidden),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(n_hidden, n_hidden))\n",
    "    # Decoder network\n",
    "    decoder = nn.Sequential(\n",
    "                nn.Linear(n_latent, n_hidden),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(n_hidden, nin * n_classes),\n",
    "                nn.Sigmoid())\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the reconstruction error\n",
    "\n",
    "In the definition of the `VAE` class, we directly included the computation of the $D_{KL}$ term to regularize our latent space. However, remember that the complete loss of equation (4) also contains a *reconstruction loss* which compares our reconstructed output to the original data. \n",
    "\n",
    "While there are several options to compare the error between two elements, there are usually two preferred choices among the generative literature depending on how we consider our problem\n",
    "1. If we consider each dimension (pixel) to be a binary unit (following a Bernoulli distribution), we can rely on the `binary cross entropy` between the two distributions\n",
    "2. If we turn our problem to a set of classifications, where each dimension can belong to a given set of *intensity classes*, then we can compute the `multinomial loss` between the two distributions\n",
    "\n",
    "In the following, we define both error functions and regroup them in the `reconstruction_loss` call (depending on the `num_classes` considered). However, as the `multinomial loss` requires a large computational overhead, and for the sake of simplicity, we will train all our first models by relying on the `binary cross entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_loss(x_tilde, x):\n",
    "    return F.binary_cross_entropy(x_tilde, x, reduction='none').sum(dim = 0)\n",
    "\n",
    "def multinomial_loss(x_logit, x):\n",
    "    batch_size = x.shape[0]\n",
    "    # Reshape input\n",
    "    x_logit = x_logit.view(batch_size, num_classes, x.shape[1], x.shape[2], x.shape[3])\n",
    "    # Take softmax\n",
    "    x_logit = F.log_softmax(x_logit, 1)\n",
    "    # make integer class labels\n",
    "    target = (x * (num_classes - 1)).long()\n",
    "    # computes cross entropy over all dimensions separately:\n",
    "    ce = F.nll_loss(x_logit, target, weight=None, reduction='none')\n",
    "    return ce.sum(dim = 0)*100\n",
    "\n",
    "def reconstruction_loss(x_tilde, x, num_classes=1, average=True):\n",
    "    if (num_classes == 1):\n",
    "        loss = binary_loss(x_tilde, x.view(x.size(0), -1))\n",
    "    else:\n",
    "        loss = multinomial_loss(x_tilde, x)\n",
    "    if (average):\n",
    "        loss = loss.sum() / x.size(0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a VAE on a real dataset\n",
    "\n",
    "For this tutorial, we are going to take a quick shot at a real-life problem by trying to train our VAEs on the `FashionMNIST` dataset. This dataset can be natively used in PyTorch by relying on the `torchvision.datasets` classes as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "tens_t = transforms.ToTensor()\n",
    "train_dset = datasets.FashionMNIST('./data', train=True, download=True, transform=tens_t)\n",
    "train_loader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_dset = datasets.FashionMNIST('./data', train=False, transform=tens_t)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FashionMNIST` dataset is composed of simple 28x28 black and white images of different items of clothings (such as shoes, bags, pants and shirts). We put a simple function here to display one batch of the test set (note that we keep a fixed batch from the test set in order to evaluate the different variations that we will try in this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(batch, nslices=8):\n",
    "    # Create one big image for plot\n",
    "    img = np.zeros(((batch.shape[2] + 1) * nslices, (batch.shape[3] + 1) * nslices))\n",
    "    for b in range(batch.shape[0]):\n",
    "        row = int(b / nslices); col = int(b % nslices)\n",
    "        r_p = row * batch.shape[2] + row; c_p = col * batch.shape[3] + col\n",
    "        img[r_p:(r_p+batch.shape[2]),c_p:(c_p+batch.shape[3])] = torch.sum(batch[b], 0)\n",
    "    im = plt.imshow(img, cmap='Greys', interpolation='nearest'),\n",
    "    return im\n",
    "# Select a random set of fixed data\n",
    "fixed_batch, fixed_targets = next(iter(test_loader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_batch(fixed_batch);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now based on our proposed implementation, the optimization aspects are defined in a very usual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bernoulli or Multinomial loss\n",
    "num_classes = 1\n",
    "# Number of hidden and latent\n",
    "n_hidden = 512\n",
    "n_latent = 2\n",
    "# Compute input dimensionality\n",
    "nin = fixed_batch.shape[2] * fixed_batch.shape[3]\n",
    "# Construct encoder and decoder\n",
    "encoder, decoder = construct_encoder_decoder(nin, n_hidden = n_hidden, n_latent = n_latent, n_classes = num_classes)\n",
    "# Build the VAE model\n",
    "model = VAE(encoder, decoder, n_hidden, n_latent)\n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99995)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that is left to do is train the model. We define here a `train_vae` function that we will reuse along the future implementations and variations of VAEs and flows. Note that this function is set to run for only a very few number of `epochs` and also most importantly, *only considers a subsample of the full dataset at each epoch*. This option is just here so that you can test the different models very quickly on any CPU or laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, optimizer, scheduler, train_loader, fixed_batch, model_name='basic', epochs=50, plot_it=1, subsample=5000, flatten=True):\n",
    "    # Losses curves\n",
    "    losses = torch.zeros(epochs, 2)\n",
    "    # Beta-warmup\n",
    "    beta = 0\n",
    "    # Plotting\n",
    "    ims = []\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    # Main optimization loop\n",
    "    for it in range(epochs):\n",
    "        it_loss = torch.Tensor([2])\n",
    "        # Update our beta\n",
    "        beta = 1. * (it / float(epochs))\n",
    "        n_batch = 0.\n",
    "        # Evaluate loss and backprop\n",
    "        for batch_idx, (x, _) in enumerate(train_loader):\n",
    "            if (batch_idx * batch_size) > subsample:\n",
    "                break\n",
    "            # Flatten input data\n",
    "            if (flatten):\n",
    "                x = x.view(-1, nin)\n",
    "            # Pass through VAE\n",
    "            x_tilde, loss_latent = model(x)\n",
    "            # Compute reconstruction loss\n",
    "            loss_recons = reconstruction_loss(x_tilde, x, num_classes)\n",
    "            # Evaluate loss and backprop\n",
    "            loss = loss_recons + (beta * loss_latent)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            losses[it, 0] += loss_recons.item()\n",
    "            losses[it, 1] += loss_latent.item()\n",
    "            n_batch += 1.\n",
    "        losses[it, :] /= n_batch\n",
    "        if (it % plot_it == 0):\n",
    "            # Encode our fixed batch\n",
    "            samples = fixed_batch\n",
    "            if (flatten):\n",
    "                samples = fixed_batch.view(-1, nin)\n",
    "            x_tilde, _ = model(samples)\n",
    "            if (num_classes > 1):\n",
    "                # Find largest class logit\n",
    "                tmp = x_tilde.view(-1, num_classes, *x[0].shape[1:]).max(dim=1)[1]\n",
    "                x_tilde = tmp.float() / (num_classes - 1.)\n",
    "            ims.append(plot_batch(x_tilde.detach().view_as(fixed_batch)))\n",
    "            plt.title('Iter.%i'%(it), fontsize=15);\n",
    "    # Generate our animation\n",
    "    anim = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
    "    HTML(anim.to_html5_video())\n",
    "    anim.save(\"vae_\" + model_name + \".mp4\")\n",
    "    return losses\n",
    "            \n",
    "# Launch our optimization\n",
    "losses_kld = train_vae(model, optimizer, scheduler, train_loader, fixed_batch, model_name='basic', epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating generative models\n",
    "\n",
    "In order to evaluate our upcoming generative models, we will rely on the computation of the Negative Log-Likelihood. This code for the following `evaluate_nll_bpd` is inspired by the [Sylvester flow repository](https://github.com/riannevdberg/sylvester-flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "def evaluate_nll_bpd(data_loader, model, batch = 500, R = 5):\n",
    "    model.eval()\n",
    "    # Set of likelihood tests\n",
    "    likelihood_test = []\n",
    "    # Go through dataset\n",
    "    for batch_idx, (x, _) in enumerate(data_loader):\n",
    "        for j in range(x.shape[0]):\n",
    "            a = []\n",
    "            for r in range(0, R):\n",
    "                cur_x = x[j].unsqueeze(0)\n",
    "                # Repeat it as batch\n",
    "                x = cur_x.expand(batch, *cur_x.size()[1:]).contiguous()\n",
    "                x = x.view(batch, -1)\n",
    "                x_tilde, kl_div = model(x)\n",
    "                rec = reconstruction_loss(x_tilde, x, average=False)\n",
    "                a_tmp = (rec + kl_div)\n",
    "                a.append(- a_tmp.cpu().data.numpy())\n",
    "            # calculate max\n",
    "            likelihood_x = logsumexp(a)\n",
    "            likelihood_test.append(likelihood_x - np.log(len(a)))\n",
    "    likelihood_test = np.array(likelihood_test)\n",
    "    nll = - np.mean(likelihood_test)\n",
    "    # Compute the bits per dim (but irrelevant for binary data)\n",
    "    bpd = nll / (np.prod(nin) * np.log(2.))\n",
    "    return nll, bpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our VAE model more formally as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final loss\n",
    "plt.figure()\n",
    "plt.plot(losses_kld[:, 0].numpy());\n",
    "# Evaluate log-likelihood and bits per dim\n",
    "nll, _ = evaluate_nll_bpd(test_loader, model)\n",
    "print('Negative Log-Likelihood : ' + str(nll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of VAEs\n",
    "\n",
    "Although VAEs are extremely powerful tools, they still have some limitations. Here we list the three most important and known limitations (all of them are still debated and topics of active research). \n",
    "1. **Blurry reconstructions.** As can be witnessed directly in the results of the previous vanilla VAE implementation, the reconstructions appear to be blurry. The precise origin of this phenomenon is still debated, but the proposed explanation are\n",
    "    1. The use of the KL regularization\n",
    "    2. High variance regions of the latent space\n",
    "    3. The reconstruction criterion (expectation)\n",
    "    4. The use of simplistic latent distributions\n",
    "2. **Posterior collapse.** The previous *blurry reconstructions* issue can be mitigated by using a more powerful decoder. However, relying on a decoder with a large capacity causes the phenomenon of *posterior collapse* where the latent space becomes useless. A nice intuitive explanation can be found [here](https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/)\n",
    "3. **Simplistic Gaussian approximation**. In the derivation of the VAE objective, recall that the KL divergence term needs to be computed analytically. Therefore, this forces us to rely on quite simplistic families. However, the Gaussian family might be too simplistic to model real world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the present tutorial, we show how normalizing flows can be used to mostly solve the third limitation, while also adressing the two first problems. Indeed, we will see that normalizing flows also lead to sharper reconstructions and also act on preventing posterior collapse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vae\"></a>\n",
    "## Normalizing flows and VAEs\n",
    "\n",
    "As we have seen in the previous section, one of the main limitations of VAEs is that they rely on a very simple family of approximations. In the original paper of Rezende [1], normalizing flows are used in order to complexify the posterior distribution of VAEs. \n",
    "\n",
    "### Flow-based free energy bound\n",
    "\n",
    "If we parameterize the approximate posterior distribution with a flow of length $K$, $q_{\\phi}(\\mathbf{z}\\vert\\mathbf{x}) = q_K(\\mathbf{z}_K)$, the free energy can be written as an expectation over the initial distribution $q_0(\\mathbf{z})$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{F}(\\mathbf{x}) &= \\mathbb{E}_{q_{\\phi}(\\mathbf{z}\\vert\\mathbf{x})}\\left[\n",
    "\\text{log }q_{\\phi}(\\mathbf{z}\\vert\\mathbf{x}) - \\text{log }p(\\mathbf{x},\\mathbf{z})\\right]\\\\\n",
    "&= \\mathbb{E}_{q_0(z_0)}\\left[\\text{ln }q_{K}(\\mathbf{z}_K) - \\text{log }p(\\mathbf{x},\\mathbf{z}_K)\\right] \\\\\n",
    "&= \\mathbb{E}_{q_0(z_0)}\\left[\\text{ln }q_{0}(\\mathbf{z}_0)\\right] - \\mathbb{E}_{q_0(z_0)}\\left[\\text{log }p(\\mathbf{x},\\mathbf{z}_K)\\right] - \\mathbb{E}_{q_0(z_0)}\\left[\\sum_{i=1}^{k} \\text{log} \\left|\\text{det}\\frac{\\delta f_i}{\\delta\\mathbf{z}_{i-1}}\\right|\\right] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Normalizing flows and this free energy bound can be used with any variational optimization scheme. For amortized variational inference, we construct an inference model using a deep neural network to build a mapping from the observations $\\mathbf{x}$ to the parameters of the initial density $q_0 = \\mathcal{N}(\\mu, \\sigma)$ as well as the parameters of the flow $\\lambda$.\n",
    "\n",
    "This allows us to write the complete optimization objective as\n",
    "$$\n",
    "\\mathcal{L}_{\\theta, \\phi} = \\mathbb{E}_{q_0} \\big[ \\log{ p_\\theta (\\mathbf{x|z_k}) } \\big] + \\mathbb{E}_{q_0}\\left[\\text{ln }q_{0}(\\mathbf{z}_0) - \\text{log }p(\\mathbf{z}_K)\\right] - \\mathbb{E}_{q_0}\\left[\\sum_{i=1}^{k} \\text{log} \\left|\\text{det}\\frac{\\delta f_i}{\\delta\\mathbf{z}_{i-1}}\\right|\\right] \n",
    "$$\n",
    "   which can be optimized with stochastic gradient descent since $q_0$ is taken to be a Gaussian from which we can easily sample. However, now the final latent samples $\\mathbf{z}_k$ will be drawn from a much more complex distribution. Also note that the previous KL criterion is replaced by two regularizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flows as posterior\n",
    "\n",
    "In this first implementation of VAEs augmented with normalizing flows, we simply add a flow after the prior sampling. This is implemented in the following `VAENormalizingFlow` class. Note that the computation of different parts of the latent regularizations and the log determinants is performed explicitly here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAENormalizingFlow(VAE):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, flow, encoder_dims, latent_dims):\n",
    "        super(VAENormalizingFlow, self).__init__(encoder, decoder, encoder_dims, latent_dims)\n",
    "        self.flow = flow\n",
    "\n",
    "    def latent(self, x, z_params):\n",
    "        n_batch = x.size(0)\n",
    "        # Retrieve set of parameters\n",
    "        mu, sigma = z_params\n",
    "        # Re-parametrize a Normal distribution\n",
    "        q = distrib.Normal(torch.zeros(mu.shape[1]), torch.ones(sigma.shape[1]))\n",
    "        # Obtain our first set of latent points\n",
    "        z_0 = (sigma * q.sample((n_batch, ))) + mu\n",
    "        # Complexify posterior with flows\n",
    "        z_k, list_ladj = self.flow(z_0)\n",
    "        # ln p(z_k) \n",
    "        log_p_zk = -0.5 * z_k * z_k\n",
    "        # ln q(z_0)\n",
    "        log_q_z0 = -0.5 * (sigma.log() + (z_0 - mu) * (z_0 - mu) * sigma.reciprocal())\n",
    "        #  ln q(z_0) - ln p(z_k)\n",
    "        logs = (log_q_z0 - log_p_zk).sum()\n",
    "        # Add log determinants\n",
    "        ladj = torch.cat(list_ladj)\n",
    "        # ln q(z_0) - ln p(z_k) - sum[log det]\n",
    "        logs -= torch.sum(ladj)\n",
    "        return z_k, (logs / float(n_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this model behaves almost exactly like the Vanilla `VAE`. However, we now complexify the latent distribution with a given `flow` and then replace the KL divergence with the regularization based on the variational free energy. Note also that we rely on the implementation of `Flow` from the previous tutorial, where each flow optimizes its own parameters. Therefore, we can simply optimize this model similarily as the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bernoulli or Multinomial loss\n",
    "num_classes = 1\n",
    "# Number of hidden and latent\n",
    "n_hidden = 512\n",
    "n_latent = 2\n",
    "# Our MLP blocks\n",
    "block_planar = [PlanarFlow]\n",
    "# Create normalizing flow\n",
    "flow = NormalizingFlow(dim=n_latent, blocks=block_planar, flow_length=16, density=distrib.MultivariateNormal(torch.zeros(n_latent), torch.eye(n_latent)))\n",
    "# Construct encoder and decoder\n",
    "encoder, decoder = construct_encoder_decoder(nin, n_hidden = n_hidden, n_latent = n_latent, n_classes = num_classes)\n",
    "# Create VAE with planar flows\n",
    "model_flow = VAENormalizingFlow(encoder, decoder, flow, n_hidden, n_latent)\n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(model_flow.parameters(), lr=1e-3)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99995)\n",
    "# Launch our optimization\n",
    "losses_flow = train_vae(model_flow, optimizer, scheduler, train_loader, fixed_batch, model_name='flow', epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare this improved VAE model with a normalizing flow to the original vanilla VAE on our log-likelihood criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.figure()\n",
    "plt.plot(losses_flow[:, 0].numpy());\n",
    "# Evaluate log-likelihood and bits per dim\n",
    "nll, _ = evaluate_nll_bpd(test_loader, model_flow)\n",
    "print('Negative Log-Likelihood : ' + str(nll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amortized inference.\n",
    "\n",
    "Although the previous implementation of `VAENormalizingFlow` seems to work well, the parameters of the flows are optimized separately from the encoder function. However, when performing amortized inference for normalizing flows, the flow parameters determine the final distribution. Therefore, the parameters should also be considered functions of the datapoint $\\mathbf{x}$.\n",
    "\n",
    "In the original paper [1] and most implementations, the parameters of the flows are directly produced by the encoder network. This allow to simplify the computations, while increasing the correlations of these parameters to the input.\n",
    "\n",
    "To do so, we are going to slightly modify our base implementation of the global `NormalizingFlow`, by providing two supplementary functions\n",
    "1. `set_parameters` allow to update the parameters of all the flows contained in this class\n",
    "2. `n_parameters` returns the total number of parameters present in the sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main class for normalizing flow\n",
    "class NormalizingFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, blocks, flow_length, density):\n",
    "        super().__init__()\n",
    "        biject = []\n",
    "        self.n_params = []\n",
    "        for f in range(flow_length):\n",
    "            for b_flow in blocks:\n",
    "                cur_block = b_flow(dim)\n",
    "                biject.append(cur_block)\n",
    "                self.n_params.append(cur_block.n_parameters())\n",
    "        self.transforms = transform.ComposeTransform(biject)\n",
    "        self.bijectors = nn.ModuleList(biject)\n",
    "        self.base_density = density\n",
    "        self.final_density = distrib.TransformedDistribution(density, self.transforms)\n",
    "        self.log_det = []\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.log_det = []\n",
    "        # Applies series of flows\n",
    "        for b in range(len(self.bijectors)):\n",
    "            self.log_det.append(self.bijectors[b].log_abs_det_jacobian(z))\n",
    "            z = self.bijectors[b](z)\n",
    "        return z, self.log_det\n",
    "        \n",
    "    def n_parameters(self):\n",
    "        return sum(self.n_params)\n",
    "    \n",
    "    def set_parameters(self, params):\n",
    "        param_list = params.split(self.n_params, dim = 1)\n",
    "        # Applies series of flows\n",
    "        for b in range(len(self.bijectors)):\n",
    "            self.bijectors[b].set_parameters(param_list[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically for a given `Flow` operation (here we only modify the `PlanarFlow`, but this can be quite simply transposed), we are also adding two functions to obtain the number of parameters (summed dimensionnality of all tensors) and set their values. Here, we also change slightly the operation performed (compared to the previous tutorials), so that the flow parameters are actually *dependent on the input*. Therefore, the tensors will now be of dimensionality `batch_size x latent_dims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.weight = []\n",
    "        self.scale = []\n",
    "        self.bias = []\n",
    "        self.dim = dim\n",
    "\n",
    "    def _call(self, z):\n",
    "        z = z.unsqueeze(2)\n",
    "        f_z = torch.bmm(self.weight, z) + self.bias\n",
    "        return (z + self.scale * torch.tanh(f_z)).squeeze(2)\n",
    "\n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        z = z.unsqueeze(2)\n",
    "        f_z = torch.bmm(self.weight, z) + self.bias\n",
    "        psi = self.weight * (1 - torch.tanh(f_z) ** 2)\n",
    "        det_grad = 1 + torch.bmm(psi, self.scale)\n",
    "        return torch.log(det_grad.abs() + 1e-9)\n",
    "    \n",
    "    def set_parameters(self, p_list):\n",
    "        self.weight = p_list[:, :self.dim].unsqueeze(1)\n",
    "        self.scale = p_list[:, self.dim:self.dim*2].unsqueeze(2)\n",
    "        self.bias = p_list[:, self.dim*2].unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "    def n_parameters(self):\n",
    "        return 2 * self.dim + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we just need to make a few adjustments to the original class in order to account for this new way of optimizing the parameters. We simply add a separate encoder for the parameters (a linear layer), and also set the parameters of the `flow` before applying it to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAENormalizingFlow(VAE):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, flow, encoder_dims, latent_dims):\n",
    "        super(VAENormalizingFlow, self).__init__(encoder, decoder, encoder_dims, latent_dims)\n",
    "        self.flow_enc = nn.Linear(encoder_dims, flow.n_parameters())\n",
    "        self.flow = flow\n",
    "        self.apply(self.init_parameters)\n",
    "        self.flow_enc.weight.data.uniform_(-0.01, 0.01)\n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        flow_params = self.flow_enc(x)\n",
    "        return mu, sigma, flow_params\n",
    "\n",
    "    def latent(self, x, z_params):\n",
    "        n_batch = x.size(0)\n",
    "        # Split the encoded values to retrieve flow parameters\n",
    "        mu, sigma, flow_params = z_params\n",
    "        # Re-parametrize a Normal distribution\n",
    "        q = distrib.Normal(torch.zeros(mu.shape[1]), torch.ones(sigma.shape[1]))\n",
    "        # Obtain our first set of latent points\n",
    "        z_0 = (sigma * q.sample((n_batch, ))) + mu\n",
    "        # Update flows parameters\n",
    "        self.flow.set_parameters(flow_params)\n",
    "        # Complexify posterior with flows\n",
    "        z_k, list_ladj = self.flow(z_0)\n",
    "        # ln p(z_k) \n",
    "        log_p_zk = -0.5 * z_k * z_k\n",
    "        # ln q(z_0)\n",
    "        log_q_z0 = -0.5 * (sigma.log() + (z_0 - mu) * (z_0 - mu) * sigma.reciprocal())\n",
    "        #  ln q(z_0) - ln p(z_k)\n",
    "        logs = (log_q_z0 - log_p_zk).sum()\n",
    "        # Add log determinants\n",
    "        ladj = torch.cat(list_ladj)\n",
    "        # ln q(z_0) - ln p(z_k) - sum[log det]\n",
    "        logs -= torch.sum(ladj)\n",
    "        return z_k, (logs / float(n_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we re-optimize this modified `VAENormalizingFlow` class with the exact same procedure and hyper-parameters as the previous implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bernoulli or Multinomial loss\n",
    "num_classes = 1\n",
    "# Number of hidden and latent\n",
    "n_hidden = 512\n",
    "n_latent = 2\n",
    "# Our MLP blocks\n",
    "block_planar = [PlanarFlow]\n",
    "# Create normalizing flow\n",
    "flow = NormalizingFlow(dim=n_latent, blocks=block_planar, flow_length=16, density=distrib.MultivariateNormal(torch.zeros(n_latent), torch.eye(n_latent)))\n",
    "# Construct encoder and decoder\n",
    "encoder, decoder = construct_encoder_decoder(nin, n_hidden = n_hidden, n_latent = n_latent, n_classes = num_classes)\n",
    "# Create VAE with planar flows\n",
    "model_flow_p = VAENormalizingFlow(encoder, decoder, flow, n_hidden, n_latent)\n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(model_flow_p.parameters(), lr=1e-3)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "# Launch our optimization\n",
    "losses_flow_param = train_vae(model_flow_p, optimizer, scheduler, train_loader, fixed_batch, model_name='flow_params', epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this implementation regarding the log-likelihood and training curves, we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.figure()\n",
    "plt.plot(losses_flow_param[:, 0].numpy());\n",
    "# Evaluate log-likelihood and bits per dim\n",
    "nll, bpd = evaluate_nll_bpd(test_loader, model_flow_p)\n",
    "print('Negative Log-Likelihood : ' + str(nll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the latent spaces of different models\n",
    "\n",
    "One of the key aspect in the difference between the vanilla VAE and the VAE with normalizing flow is the treatment of the latent space. Hence a good way of assessing the impact of our flows is to check how the flow process the latent points. Here, we perform some visualisations of the latent spaces obtained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 6))\n",
    "models = {'vanilla':(model, ax1), 'planar indep.':(model_flow, ax2), 'planar params':(model_flow_p, ax3)}\n",
    "for name, (cur_model, ax) in models.items():\n",
    "    final_z = []\n",
    "    final_classes = []\n",
    "    for batch_idx, (x, c) in enumerate(train_loader):\n",
    "        if (x.shape[0] != 64):\n",
    "            break\n",
    "        x = (x.view(x.shape[0], -1) * 2) - 1\n",
    "        # Not exact but just consider mean for laziness\n",
    "        cur_mu, cur_sig, *params = cur_model.encode(x)\n",
    "        q = distrib.Normal(torch.zeros(cur_mu.shape[1]), torch.ones(cur_sig.shape[1]))\n",
    "        cur_z = (cur_sig * q.sample((64, ))) + cur_mu\n",
    "        if (hasattr(cur_model, 'flow')):\n",
    "            if (name == 'planar params'):\n",
    "                cur_model.flow.set_parameters(params[0])\n",
    "            cur_z, _ = cur_model.flow(cur_z)\n",
    "        final_z.append(cur_z.detach())\n",
    "        final_classes.append(c)\n",
    "    final_z = torch.cat(final_z)\n",
    "    final_classes = torch.cat(final_classes)\n",
    "    # Create PCA and apply it to our data\n",
    "    pca = decomposition.PCA(n_components=2)\n",
    "    pca.fit(final_z)\n",
    "    z_pca = pca.transform(final_z)\n",
    "    ax.scatter(z_pca[:, 0], z_pca[:, 1], c=final_classes, cmap=plt.cm.nipy_spectral, edgecolor='k')\n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our implementation of normalizing flows with VAEs :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"improve\"></a>\n",
    "## Improving the quality of VAEs\n",
    "\n",
    "As we discussed in the previous section, several known issues have been reported when using the vanilla VAE implementation. We listed some of the major issues as being\n",
    "1. **Blurry reconstructions.** \n",
    "2. **Posterior collapse.**\n",
    "3. **Simplistic Gaussian approximation**.\n",
    "\n",
    "Here, we discuss some recent developments that were proposed in the VAE literature and simple adjustments that can be made to (at least partly) alleviate these issues. However, note that some more advanced proposals such as PixelVAE [5](#reference1) and VQ-VAE [6](#reference1) can lead to wider increases in quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the bluriness of reconstructions\n",
    "\n",
    "In this tutorial, we relied on extremely simple decoder functions, to show how we could easily define VAEs and normalizing flows together. However, the capacity of the decoder obviously directly influences the quality of the final reconstruction. Therefore, we could address this issue naively by using deep networks and of course convolutional layers as we are currently dealing with images.\n",
    "\n",
    "Here, we first introduce two layers based on *gated convolutions*. These will be our basis for constructing the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedConv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel, stride, pad, dilate=1, act=torch.relu):\n",
    "        super(GatedConv2d, self).__init__()\n",
    "        self.activation = act\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.h = nn.Conv2d(in_c, out_c, kernel, stride, pad, dilate)\n",
    "        self.g = nn.Conv2d(in_c, out_c, kernel, stride, pad, dilate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.h(x))\n",
    "        g = self.sigmoid(self.g(x))\n",
    "        return h * g\n",
    "\n",
    "class GatedConvTranspose2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel, stride, pad, out_pad=0, dilate=1, act=torch.relu):\n",
    "        super(GatedConvTranspose2d, self).__init__()\n",
    "        self.activation = act\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.h = nn.ConvTranspose2d(in_c, out_c, kernel, stride, pad, out_pad, dilation=dilate)\n",
    "        self.g = nn.ConvTranspose2d(in_c, out_c, kernel, stride, pad, out_pad, dilation=dilate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.h(x))\n",
    "        g = self.sigmoid(self.g(x))\n",
    "        return h * g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a more complex encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encoder_decoder_complex(nin, n_latent = 16, n_hidden = 512, n_params = 0, n_classes = 1):\n",
    "    # Encoder network\n",
    "    encoder = nn.Sequential(\n",
    "                GatedConv2d(1, 32, 5, 1, 2),\n",
    "                GatedConv2d(32, 32, 5, 2, 2),\n",
    "                GatedConv2d(32, 64, 5, 1, 2),\n",
    "                GatedConv2d(64, 64, 5, 2, 2),\n",
    "                GatedConv2d(64, 64, 5, 1, 2),\n",
    "                GatedConv2d(64, n_hidden, 7, 1, 0))\n",
    "    # Decoder network\n",
    "    decoder = nn.Sequential(\n",
    "                GatedConvTranspose2d(n_latent, 64, 7, 1, 0),\n",
    "                GatedConvTranspose2d(64, 64, 5, 1, 2),\n",
    "                GatedConvTranspose2d(64, 32, 5, 2, 2, 1),\n",
    "                GatedConvTranspose2d(32, 32, 5, 1, 2),\n",
    "                GatedConvTranspose2d(32, 32, 5, 2, 2, 1),\n",
    "                GatedConvTranspose2d(32, 32, 5, 1, 2),\n",
    "                nn.Conv2d(32, 256, 5, 1, 2),\n",
    "                nn.Conv2d(256, 1 * num_classes, 1, 1, 0))\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preventing posterior collapse with Wasserstein-VAE-MMD (InfoVAE)\n",
    "\n",
    "As we discussed earlier, the reason behind posterior collapse mostly relates to the KL divergence criterion (a nice intuitive explanation can be found [here](https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/). This can be mitigated by relying on a different criterion, such as regularizing the latent distribution by using the *Maximum Mean Discrepancy* (MMD) instead of the KL divergence. This model was independently proposed as the *InfoVAE* and later also as the *Wasserstein-VAE*.\n",
    "\n",
    "Here we provide a simple implementation of the `InfoVAEMMD` class based on our previous implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel(x, y):\n",
    "    x_size = x.size(0)\n",
    "    y_size = y.size(0)\n",
    "    dim = x.size(1)\n",
    "    x = x.unsqueeze(1)\n",
    "    y = y.unsqueeze(0)\n",
    "    tiled_x = x.expand(x_size, y_size, dim)\n",
    "    tiled_y = y.expand(x_size, y_size, dim)\n",
    "    kernel_input = (tiled_x - tiled_y).pow(2).mean(2)/float(dim)\n",
    "    return torch.exp(-kernel_input) \n",
    "\n",
    "def compute_mmd(x, y):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()\n",
    "    return mmd\n",
    "\n",
    "class InfoVAEMMD(VAE):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(InfoVAEMMD, self).__init__(encoder, decoder)\n",
    "        \n",
    "    def latent(self, x, z_params):\n",
    "        n_batch = x.size(0)\n",
    "        mu, sigma = z_params.chunk(2, dim=1)\n",
    "        # Re-parametrize\n",
    "        q = distrib.Normal(torch.zeros(mu.shape[1]), torch.ones(sigma.shape[1]))\n",
    "        z = (sigma * q.sample((n_batch, ))) + mu\n",
    "        # Sample from the z prior\n",
    "        z_prior = q.sample((n_batch, ))\n",
    "        # Compute MMD divergence\n",
    "        mmd_dist = compute_mmd(z, z_prior)\n",
    "        return z, mmd_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexifying the posterior with flows\n",
    "\n",
    "As this was the central topic of this tutorial, we will not go through the explanation again. However, as we will now be relying on convolutional layers instead of linear ones, we just need to make small changes in our encoding and decoding functions in order to control the shape of the different tensors. This is performed in the following class by simply adding two `view` operations in the `encode` and `decode` functions respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAENormalizingFlow(VAE):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, flow, encoder_dims, latent_dims):\n",
    "        super(VAENormalizingFlow, self).__init__(encoder, decoder, encoder_dims, latent_dims)\n",
    "        self.flow_enc = nn.Linear(encoder_dims, flow.n_parameters())\n",
    "        self.flow = flow\n",
    "        self.apply(self.init_parameters)\n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        flow_params = self.flow_enc(x)\n",
    "        return mu, sigma, flow_params\n",
    "            \n",
    "    def decode(self, z):\n",
    "        z = z.view(z.shape[0], z.shape[1], 1, 1)\n",
    "        x_tilde = self.decoder(z)\n",
    "        return x_tilde\n",
    "\n",
    "    def latent(self, x, z_params):\n",
    "        n_batch = x.size(0)\n",
    "        # Split the encoded values to retrieve flow parameters\n",
    "        mu, sigma, flow_params = z_params\n",
    "        # Re-parametrize a Normal distribution\n",
    "        q = distrib.Normal(torch.zeros(mu.shape[1]), torch.ones(sigma.shape[1]))\n",
    "        # Obtain our first set of latent points\n",
    "        z_0 = (sigma * q.sample((n_batch, ))) + mu\n",
    "        # Update flows parameters\n",
    "        self.flow.set_parameters(flow_params)\n",
    "        # Complexify posterior with flows\n",
    "        z_k, list_ladj = self.flow(z_0)\n",
    "        # ln p(z_k) \n",
    "        log_p_zk = torch.sum(-0.5 * z_k * z_k, dim=1)\n",
    "        # ln q(z_0)  (not averaged)\n",
    "        log_q_z0 = torch.sum(-0.5 * (sigma.log() + (z_0 - mu) * (z_0 - mu) * sigma.reciprocal()), dim=1)\n",
    "        #  ln q(z_0) - ln p(z_k)\n",
    "        logs = (log_q_z0 - log_p_zk).sum()\n",
    "        # Add log determinants\n",
    "        ladj = torch.cat(list_ladj, dim=1)\n",
    "        # ln q(z_0) - ln p(z_k) - sum[log det]\n",
    "        logs -= torch.sum(ladj)\n",
    "        return z_k, (logs / float(n_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Here we combine all these ideas (except for the MMD, which is not adequate as the flow definition already regularizes the latent space without the KL divergence) to perform a more advanced optimization of the dataset. Hence, we will rely on the complex encoder and decoder with gated convolutions, the multinomial loss and the normalizing flows in order to improve the overall quality of our reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of latent space\n",
    "n_latent = 16\n",
    "# Number of hidden units\n",
    "n_hidden = 256\n",
    "# Our MLP blocks\n",
    "block_planar = [PlanarFlow]\n",
    "# Create normalizing flow\n",
    "flow = NormalizingFlow(dim=n_latent, blocks=block_planar, flow_length=16, density=distrib.MultivariateNormal(torch.zeros(n_latent), torch.eye(n_latent)))\n",
    "# Rely on Bernoulli or multinomial\n",
    "num_classes = 128\n",
    "# Construct encoder and decoder\n",
    "encoder, decoder = construct_encoder_decoder_complex(nin, n_hidden = n_hidden, n_latent = n_latent, n_classes = num_classes)\n",
    "# Create VAE with planar flows\n",
    "model_flow_p = VAENormalizingFlow(encoder, decoder, flow, n_hidden, n_latent)\n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(model_flow_p.parameters(), lr=1e-3)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99995)\n",
    "# Launch our optimization\n",
    "losses_flow_param = train_vae(model_flow_p, optimizer, scheduler, train_loader, fixed_batch, model_name='flow_complex', epochs=200, flatten=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NB*: It seems that the multinomial version have a hard time converging. Although I only let this run for 200 epochs and only for a subsampling of 5000 examples, it might need more time, but this might also come from a mistake somewhere in my code ... If you spot something odd please let me know :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<a id=\"reference1\"></a>\n",
    "[1] Rezende, Danilo Jimenez, and Shakir Mohamed. \"Variational inference with normalizing flows.\" _arXiv preprint arXiv:1505.05770_ (2015). [link](http://arxiv.org/pdf/1505.05770)\n",
    "\n",
    "[2] Kingma, Diederik P., Tim Salimans, and Max Welling. \"Improving Variational Inference with Inverse Autoregressive Flow.\" _arXiv preprint arXiv:1606.04934_ (2016). [link](https://arxiv.org/abs/1606.04934)\n",
    "\n",
    "[3] Kingma, D. P., & Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. (2013). [link](https://arxiv.org/pdf/1312.6114)\n",
    "\n",
    "[4] Rezende, D. J., Mohamed, S., & Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082. (2014). [link](https://arxiv.org/pdf/1401.4082)\n",
    "\n",
    "[5] Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D., & Courville, A. (2016). Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013. [link](https://arxiv.org/pdf/1611.05013)\n",
    "\n",
    "[6] Van den Oord, A., & Vinyals, O. (2017). Neural discrete representation learning. In NIPS 2017 (pp. 6306-6315). [link](http://papers.nips.cc/paper/7210-neural-discrete-representation-learning.pdf)\n",
    "\n",
    "### Inspirations and resources\n",
    "\n",
    "https://blog.evjang.com/2018/01/nf1.html  \n",
    "https://github.com/ex4sperans/variational-inference-with-normalizing-flows  \n",
    "https://akosiorek.github.io/ml/2018/04/03/norm_flows.html  \n",
    "https://github.com/abdulfatir/normalizing-flows  \n",
    "https://github.com/riannevdberg/sylvester-flows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
