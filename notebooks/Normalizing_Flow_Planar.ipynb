{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing flow demos. \n",
    "References:\n",
    "* https://github.com/acids-ircam/pytorch_flows/blob/master/flows_01.ipynb\n",
    "* https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html\n",
    "* https://blog.evjang.com/2018/01/nf1.html\n",
    "* https://blog.evjang.com/2018/01/nf1.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torch.distributions.transforms as T\n",
    "import torch.distributions as D\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grids of points for plotting\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "z = np.array(np.meshgrid(x, x)).transpose(1, 2, 0)\n",
    "z = np.reshape(z, [z.shape[0] * z.shape[1], -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = D.Normal(0, 1)\n",
    "td_exp = T.ExpTransform()\n",
    "q1 = D.TransformedDistribution(q0, td_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled densities\n",
    "q0_samples = q0.sample((int(1e4), ))\n",
    "q1_samples = q1.sample((int(1e4), ))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 8, 4\n",
    "sns.set()\n",
    "fig, axis = plt.subplots(nrows=1, ncols=2)    \n",
    "sns.distplot(q0_samples, ax = axis[0])\n",
    "sns.distplot(q1_samples, ax = axis[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True densities\n",
    "q0_density = torch.exp(q0.log_prob(torch.Tensor(x))).numpy()\n",
    "q1_density = torch.exp(q1.log_prob(torch.Tensor(x))).numpy()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 8, 4\n",
    "sns.set()\n",
    "fig, axis = plt.subplots(nrows=1, ncols=2)    \n",
    "axis[0].plot(x, q0_density)\n",
    "axis[1].plot(x, q1_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = D.MultivariateNormal(torch.ones(2), torch.eye(2))\n",
    "\n",
    "# exp transform\n",
    "f1 = T.ExpTransform()\n",
    "q1 = D.TransformedDistribution(q0, f1)\n",
    "\n",
    "# Affine transform\n",
    "f2 = T.AffineTransform(2, torch.Tensor([0.2, 1.5]))\n",
    "\n",
    "# Dist q2 as transform on q0 via f1, f2\n",
    "q2 = D.TransformedDistribution(q0, [f1, f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 12, 4\n",
    "fig, axis = plt.subplots(nrows=1, ncols=3)\n",
    "axis[0].hexbin(z[:, 0], z[:, 1], torch.exp(q0.log_prob(torch.Tensor(z))), cmap='rainbow')\n",
    "axis[1].hexbin(z[:, 0], z[:, 1], torch.exp(q1.log_prob(torch.Tensor(z))), cmap='rainbow')\n",
    "axis[2].hexbin(z[:, 0], z[:, 1], torch.exp(q2.log_prob(torch.Tensor(z))), cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planar flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(T.Transform):\n",
    "    def __init__(self, weight, scale, bias):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.bijective = False\n",
    "        self.weight = weight\n",
    "        self.scale = scale\n",
    "        self.bias = bias\n",
    "    \n",
    "    # Transform classes should implement _call (forward) or _inverse()\n",
    "    def _call(self, z):\n",
    "        f_z = torch.mm(z, self.weight.t()) + self.bias\n",
    "        return z + self.scale * torch.tanh(f_z)\n",
    "    \n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        f_z = torch.mm(z, self.weight.t()) + self.bias\n",
    "        psi_z = (1 - torch.pow(torch.tanh(f_z), 2)) * self.weight\n",
    "        det_grad = 1 + torch.mm(psi_z, self.scale.t())\n",
    "        return torch.log(det_grad.abs() + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.Tensor([[3.0, 0]])\n",
    "u = torch.Tensor([[2, 0]])\n",
    "b = torch.Tensor([0])\n",
    "\n",
    "q0 = D.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "flow_0 = PlanarFlow(w, u, b)\n",
    "q1 = D.TransformedDistribution(q0, flow_0)\n",
    "\n",
    "# Show emperical samples from q1, because the planar flow isn't invertible in all regions\n",
    "# of space\n",
    "q0_samples = q0.sample((int(1e6), ))\n",
    "q1_samples = q1.sample((int(1e6), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 8, 4\n",
    "fig, axis = plt.subplots(nrows=1, ncols=2)\n",
    "axis[0].hexbin(q0_samples[:, 0], q0_samples[:, 1], cmap='rainbow')\n",
    "axis[1].hexbin(q1_samples[:, 0], q1_samples[:, 1], cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change of variables / jacobian\n",
    "q0_density = torch.exp(q0.log_prob(torch.Tensor(z)))\n",
    "\n",
    "# Apply transform to coordinates\n",
    "f_z = flow_0(torch.Tensor(z))\n",
    "\n",
    "# Obtain the density\n",
    "q1_density = q0_density.squeeze() / \\\n",
    "    np.exp(flow_0.log_abs_det_jacobian(torch.Tensor(z)).squeeze())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 8, 4\n",
    "fig, axis = plt.subplots(nrows=1, ncols=2)\n",
    "axis[0].hexbin(z[:, 0], z[:, 1], q0_density, cmap='rainbow')\n",
    "axis[1].hexbin(z[:, 0], z[:, 1], q1_density, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing normalizing flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flow, self).__init__(self)\n",
    "    \n",
    "    # init parameters\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.01, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(Flow):\n",
    "    def __init__(self, dim):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.scale = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(1))\n",
    "        self.init_parameters()\n",
    "    \n",
    "    # Transform classes should implement _call (forward) or _inverse()\n",
    "    def _call(self, z):\n",
    "        f_z = torch.mm(z, self.weight.t()) + self.bias\n",
    "        return z + self.scale * torch.tanh(f_z)\n",
    "    \n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        f_z = torch.mm(z, self.weight.t()) + self.bias\n",
    "        psi_z = (1 - torch.pow(torch.tanh(f_z), 2)) * self.weight\n",
    "        det_grad = 1 + torch.mm(psi_z, self.scale.t())\n",
    "        return torch.log(det_grad.abs() + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex density we wish to learn\n",
    "def density_ring(z):\n",
    "    z1, z2 = torch.chunk(z, chunks=2, dim=1)\n",
    "    norm = torch.sqrt(z1 ** 2 + z2 ** 2)\n",
    "    exp1 = torch.exp(-0.5 * ((z1 - 2) / 0.8) ** 2)\n",
    "    exp2 = torch.exp(-0.5 * ((z1 + 2) / 0.8) ** 2)\n",
    "    u = 0.5 * ((norm - 4) / 0.4) ** 2 - torch.log(exp1 + exp2)\n",
    "    return torch.exp(-u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 5, 5\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "z = np.array(np.meshgrid(x, x)).transpose(1, 2, 0)\n",
    "z = np.reshape(z, [z.shape[0] * z.shape[1], -1])\n",
    "plt.hexbin(z[:, 0], z[:, 1], density_ring(torch.Tensor(z)).numpy().squeeze(), cmap='rainbow')\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For approximation, chain multiple planar flows in new modulule\n",
    "class NormalizingFlow(nn.Module):\n",
    "    def __init__(self, dim, n_flow):\n",
    "        super().__init__()\n",
    "        bijectors = []\n",
    "        for flow in range(n_flow):\n",
    "            bijectors.append(PlanarFlow(dim))\n",
    "            \n",
    "        self.bijectors = nn.ModuleList(bijectors)\n",
    "        self.log_det = []\n",
    "    \n",
    "    def forward(self, z):\n",
    "        self.log_det = []\n",
    "        for bijector in self.bijectors:\n",
    "            # save log abs jac\n",
    "            self.log_det.append(bijector.log_abs_det_jacobian(z))\n",
    "            # forward\n",
    "            z = bijector(z)\n",
    "        return z, self.log_det\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalizing flow\n",
    "q0 = D.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "flow = NormalizingFlow(dim=2, n_flow=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def flow_loss(target_dist_func, zk, log_abs_jac):\n",
    "    sum_of_log_jacobians = sum(log_abs_jac)\n",
    "    return (- sum_of_log_jacobians - torch.log(func_target_dist(zk) + 1e-9)).mean()\n",
    "\n",
    "# def flow_loss(target_dist_func, p0, zk, log_abs_jac):\n",
    "#     sum_of_log_jacobians = sum(log_abs_jac)\n",
    "#     return (p0 - sum_of_log_jacobians).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_figure = 2\n",
    "\n",
    "# visualization\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "z = np.array(np.meshgrid(x, x)).transpose(1, 2, 0)\n",
    "z = np.reshape(z, [z.shape[0] * z.shape[1], -1])\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 16, 18\n",
    "plt.figure(figsize=(16, 18))\n",
    "plt.subplot(3, 4, 1)\n",
    "plt.hexbin(z[:, 0], z[:, 1], C=density_ring(torch.Tensor(z)).numpy().squeeze(), cmap='rainbow')\n",
    "plt.title('Target density', fontsize=15);\n",
    "\n",
    "# Begin distribution, Target distribution and training data\n",
    "q0 = D.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "\n",
    "# target distribution function\n",
    "target_dist_func = density_ring\n",
    "\n",
    "# Flow Model\n",
    "flow_model = NormalizingFlow(dim=2, n_flow=16, density=q0)\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(flow_model.parameters(), lr=2e-3)\n",
    "# Schedular\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "# main optimization loop\n",
    "for i in range(10001):\n",
    "    # draw sample from reference normal multivariate\n",
    "    samples = q0.sample((512, ))\n",
    "    \n",
    "    # Flow forward\n",
    "    z, log_det = flow_model(samples)\n",
    "    # zero grad\n",
    "    optimizer.zero_grad()\n",
    "    # loss\n",
    "    loss_f = flow_loss(target_dist_func, z, log_det)\n",
    "    # backprop\n",
    "    loss_f.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print('Loss (it. %i) : %f'%(i, loss_f.item()))\n",
    "        samples = q0.sample((int(1e5), ))\n",
    "        # Eval flow and plot\n",
    "        z, _ = flow_model(samples)\n",
    "        z = z.detach().numpy()\n",
    "        plt.subplot(3, 4, id_figure)\n",
    "        plt.hexbin(z[:, 0], z[:, 1], cmap='rainbow')\n",
    "        plt.title('Iter.%i'%(i), fontsize=15);\n",
    "        id_figure += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
